{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión\n",
    "En una regresión lo que queremos es obtener uno o más valores concretos, no clases. Es como si quisieramos un resultado de una operación matemática. Vemos un ejemplo de una red de regresión que queremos que aprenda a sumar 3 números. \n",
    "\n",
    "Primero generamos número aleatorios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3) (1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Generamos training set con 1000 datos. \n",
    "X1 = np.random.uniform(size = 1000) * 100\n",
    "X2 = np.random.uniform(size = 1000) * 100\n",
    "X3 = np.random.uniform(size = 1000) * 100\n",
    "X = np.transpose([X1, X2, X3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este seran nuestro valores de entrada o *inputs*. Los *outputs* de nuestro problema serán la suma de los números,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X1 + X2 + X3  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya tenemos un set de ejemplo. Volvemos a separar los datos en set de entrenamiento y set de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750,), (250,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y) \n",
    "np.shape(y_train),np.shape(y_test) # comprobamos tamaño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el objeto `MLPRegressor` para definir las características de la red. La **batch_size** no la hemos tocado antes, por defecto toman un valor que dependerá del método de optimización escogido, pero la podríamos cambiar si nos interesa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "# Escojo 3 capas de 10 neuronas\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10),max_iter=500,verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10020.40050238\n",
      "Iteration 2, loss = 9746.19371920\n",
      "Iteration 3, loss = 9473.84156895\n",
      "Iteration 4, loss = 9202.16829646\n",
      "Iteration 5, loss = 8936.30231243\n",
      "Iteration 6, loss = 8673.06099675\n",
      "Iteration 7, loss = 8414.58797085\n",
      "Iteration 8, loss = 8158.67711758\n",
      "Iteration 9, loss = 7906.31082492\n",
      "Iteration 10, loss = 7652.56100315\n",
      "Iteration 11, loss = 7406.18401139\n",
      "Iteration 12, loss = 7162.59688649\n",
      "Iteration 13, loss = 6918.53912703\n",
      "Iteration 14, loss = 6680.17128104\n",
      "Iteration 15, loss = 6441.52052095\n",
      "Iteration 16, loss = 6205.91973031\n",
      "Iteration 17, loss = 5976.07290120\n",
      "Iteration 18, loss = 5742.77546582\n",
      "Iteration 19, loss = 5518.22805612\n",
      "Iteration 20, loss = 5289.63580021\n",
      "Iteration 21, loss = 5066.00925444\n",
      "Iteration 22, loss = 4843.54076574\n",
      "Iteration 23, loss = 4620.72275385\n",
      "Iteration 24, loss = 4396.76389937\n",
      "Iteration 25, loss = 4168.82322654\n",
      "Iteration 26, loss = 3942.61594980\n",
      "Iteration 27, loss = 3716.67943924\n",
      "Iteration 28, loss = 3486.26994767\n",
      "Iteration 29, loss = 3266.07256530\n",
      "Iteration 30, loss = 3052.63701764\n",
      "Iteration 31, loss = 2839.98546872\n",
      "Iteration 32, loss = 2634.98135948\n",
      "Iteration 33, loss = 2440.46510334\n",
      "Iteration 34, loss = 2252.12837610\n",
      "Iteration 35, loss = 2074.07933895\n",
      "Iteration 36, loss = 1900.58859605\n",
      "Iteration 37, loss = 1742.07892906\n",
      "Iteration 38, loss = 1592.00046964\n",
      "Iteration 39, loss = 1450.88642080\n",
      "Iteration 40, loss = 1322.56021788\n",
      "Iteration 41, loss = 1201.54583756\n",
      "Iteration 42, loss = 1090.32070268\n",
      "Iteration 43, loss = 989.73958854\n",
      "Iteration 44, loss = 897.84164221\n",
      "Iteration 45, loss = 812.40624474\n",
      "Iteration 46, loss = 736.70349716\n",
      "Iteration 47, loss = 669.06323771\n",
      "Iteration 48, loss = 606.09347011\n",
      "Iteration 49, loss = 550.92908562\n",
      "Iteration 50, loss = 501.44841624\n",
      "Iteration 51, loss = 457.30281844\n",
      "Iteration 52, loss = 418.51609145\n",
      "Iteration 53, loss = 383.68084391\n",
      "Iteration 54, loss = 352.93963060\n",
      "Iteration 55, loss = 326.92243176\n",
      "Iteration 56, loss = 303.17069091\n",
      "Iteration 57, loss = 282.92140762\n",
      "Iteration 58, loss = 265.66616544\n",
      "Iteration 59, loss = 250.37220779\n",
      "Iteration 60, loss = 237.03024431\n",
      "Iteration 61, loss = 225.40617126\n",
      "Iteration 62, loss = 215.26299335\n",
      "Iteration 63, loss = 206.76892915\n",
      "Iteration 64, loss = 199.37408392\n",
      "Iteration 65, loss = 192.44483079\n",
      "Iteration 66, loss = 186.84549699\n",
      "Iteration 67, loss = 181.48795124\n",
      "Iteration 68, loss = 177.09281903\n",
      "Iteration 69, loss = 172.88081125\n",
      "Iteration 70, loss = 169.21500247\n",
      "Iteration 71, loss = 165.81964963\n",
      "Iteration 72, loss = 162.73184096\n",
      "Iteration 73, loss = 159.80054822\n",
      "Iteration 74, loss = 157.17690605\n",
      "Iteration 75, loss = 154.45537546\n",
      "Iteration 76, loss = 152.17723953\n",
      "Iteration 77, loss = 149.76119658\n",
      "Iteration 78, loss = 147.50463980\n",
      "Iteration 79, loss = 145.33043297\n",
      "Iteration 80, loss = 143.14567468\n",
      "Iteration 81, loss = 141.05034596\n",
      "Iteration 82, loss = 138.93672098\n",
      "Iteration 83, loss = 136.96649619\n",
      "Iteration 84, loss = 134.95965510\n",
      "Iteration 85, loss = 132.95801725\n",
      "Iteration 86, loss = 131.03869218\n",
      "Iteration 87, loss = 129.10923142\n",
      "Iteration 88, loss = 127.16564939\n",
      "Iteration 89, loss = 125.25532745\n",
      "Iteration 90, loss = 123.43037074\n",
      "Iteration 91, loss = 121.52652529\n",
      "Iteration 92, loss = 119.65018488\n",
      "Iteration 93, loss = 117.85617935\n",
      "Iteration 94, loss = 116.05596439\n",
      "Iteration 95, loss = 114.25269211\n",
      "Iteration 96, loss = 112.52763471\n",
      "Iteration 97, loss = 110.71860675\n",
      "Iteration 98, loss = 108.97263304\n",
      "Iteration 99, loss = 107.22908556\n",
      "Iteration 100, loss = 105.53215968\n",
      "Iteration 101, loss = 103.85156539\n",
      "Iteration 102, loss = 102.18443798\n",
      "Iteration 103, loss = 100.49253367\n",
      "Iteration 104, loss = 98.86093928\n",
      "Iteration 105, loss = 97.23925501\n",
      "Iteration 106, loss = 95.63797681\n",
      "Iteration 107, loss = 94.03032963\n",
      "Iteration 108, loss = 92.43235662\n",
      "Iteration 109, loss = 90.90346927\n",
      "Iteration 110, loss = 89.35864071\n",
      "Iteration 111, loss = 87.83046841\n",
      "Iteration 112, loss = 86.31390740\n",
      "Iteration 113, loss = 84.83200429\n",
      "Iteration 114, loss = 83.33505137\n",
      "Iteration 115, loss = 81.87417797\n",
      "Iteration 116, loss = 80.43362896\n",
      "Iteration 117, loss = 79.00344730\n",
      "Iteration 118, loss = 77.60377397\n",
      "Iteration 119, loss = 76.22904134\n",
      "Iteration 120, loss = 74.80901541\n",
      "Iteration 121, loss = 73.45418221\n",
      "Iteration 122, loss = 72.11896615\n",
      "Iteration 123, loss = 70.81142499\n",
      "Iteration 124, loss = 69.49916166\n",
      "Iteration 125, loss = 68.22687700\n",
      "Iteration 126, loss = 66.92500679\n",
      "Iteration 127, loss = 65.67775224\n",
      "Iteration 128, loss = 64.44312838\n",
      "Iteration 129, loss = 63.21854983\n",
      "Iteration 130, loss = 62.02320989\n",
      "Iteration 131, loss = 60.81837370\n",
      "Iteration 132, loss = 59.62050680\n",
      "Iteration 133, loss = 58.50304932\n",
      "Iteration 134, loss = 57.35137517\n",
      "Iteration 135, loss = 56.22066369\n",
      "Iteration 136, loss = 55.11647757\n",
      "Iteration 137, loss = 54.03602400\n",
      "Iteration 138, loss = 52.95948229\n",
      "Iteration 139, loss = 51.88261103\n",
      "Iteration 140, loss = 50.85305327\n",
      "Iteration 141, loss = 49.81882279\n",
      "Iteration 142, loss = 48.80515063\n",
      "Iteration 143, loss = 47.79003517\n",
      "Iteration 144, loss = 46.82735667\n",
      "Iteration 145, loss = 45.83461877\n",
      "Iteration 146, loss = 44.91878395\n",
      "Iteration 147, loss = 43.98667717\n",
      "Iteration 148, loss = 43.05965621\n",
      "Iteration 149, loss = 42.13585884\n",
      "Iteration 150, loss = 41.25905424\n",
      "Iteration 151, loss = 40.40555613\n",
      "Iteration 152, loss = 39.53210289\n",
      "Iteration 153, loss = 38.67638706\n",
      "Iteration 154, loss = 37.87371917\n",
      "Iteration 155, loss = 37.02521406\n",
      "Iteration 156, loss = 36.23543687\n",
      "Iteration 157, loss = 35.44010667\n",
      "Iteration 158, loss = 34.66698769\n",
      "Iteration 159, loss = 33.90772699\n",
      "Iteration 160, loss = 33.15282936\n",
      "Iteration 161, loss = 32.40572694\n",
      "Iteration 162, loss = 31.69043716\n",
      "Iteration 163, loss = 30.99088877\n",
      "Iteration 164, loss = 30.29351472\n",
      "Iteration 165, loss = 29.59230263\n",
      "Iteration 166, loss = 28.93510830\n",
      "Iteration 167, loss = 28.26624775\n",
      "Iteration 168, loss = 27.62107777\n",
      "Iteration 169, loss = 27.00101169\n",
      "Iteration 170, loss = 26.37716155\n",
      "Iteration 171, loss = 25.75833593\n",
      "Iteration 172, loss = 25.15444406\n",
      "Iteration 173, loss = 24.58362884\n",
      "Iteration 174, loss = 24.00207584\n",
      "Iteration 175, loss = 23.43836841\n",
      "Iteration 176, loss = 22.89049309\n",
      "Iteration 177, loss = 22.34711153\n",
      "Iteration 178, loss = 21.82650246\n",
      "Iteration 179, loss = 21.29560761\n",
      "Iteration 180, loss = 20.79988162\n",
      "Iteration 181, loss = 20.29525705\n",
      "Iteration 182, loss = 19.81167819\n",
      "Iteration 183, loss = 19.33864186\n",
      "Iteration 184, loss = 18.87661306\n",
      "Iteration 185, loss = 18.41554176\n",
      "Iteration 186, loss = 17.96740847\n",
      "Iteration 187, loss = 17.52664883\n",
      "Iteration 188, loss = 17.10189650\n",
      "Iteration 189, loss = 16.69543325\n",
      "Iteration 190, loss = 16.27605011\n",
      "Iteration 191, loss = 15.87304397\n",
      "Iteration 192, loss = 15.48768061\n",
      "Iteration 193, loss = 15.10380449\n",
      "Iteration 194, loss = 14.73013620\n",
      "Iteration 195, loss = 14.36849877\n",
      "Iteration 196, loss = 14.01194407\n",
      "Iteration 197, loss = 13.66543506\n",
      "Iteration 198, loss = 13.32582057\n",
      "Iteration 199, loss = 12.99722085\n",
      "Iteration 200, loss = 12.67126346\n",
      "Iteration 201, loss = 12.35968102\n",
      "Iteration 202, loss = 12.04660853\n",
      "Iteration 203, loss = 11.75751259\n",
      "Iteration 204, loss = 11.45189631\n",
      "Iteration 205, loss = 11.17863100\n",
      "Iteration 206, loss = 10.89042834\n",
      "Iteration 207, loss = 10.62880136\n",
      "Iteration 208, loss = 10.35381127\n",
      "Iteration 209, loss = 10.10169407\n",
      "Iteration 210, loss = 9.85025165\n",
      "Iteration 211, loss = 9.60366532\n",
      "Iteration 212, loss = 9.36641125\n",
      "Iteration 213, loss = 9.13065171\n",
      "Iteration 214, loss = 8.90629777\n",
      "Iteration 215, loss = 8.67924130\n",
      "Iteration 216, loss = 8.46943670\n",
      "Iteration 217, loss = 8.25929760\n",
      "Iteration 218, loss = 8.05299959\n",
      "Iteration 219, loss = 7.85291589\n",
      "Iteration 220, loss = 7.66227506\n",
      "Iteration 221, loss = 7.47719784\n",
      "Iteration 222, loss = 7.28876117\n",
      "Iteration 223, loss = 7.10862311\n",
      "Iteration 224, loss = 6.93408003\n",
      "Iteration 225, loss = 6.76902480\n",
      "Iteration 226, loss = 6.60402137\n",
      "Iteration 227, loss = 6.44426504\n",
      "Iteration 228, loss = 6.28888394\n",
      "Iteration 229, loss = 6.13628575\n",
      "Iteration 230, loss = 5.98916747\n",
      "Iteration 231, loss = 5.84940726\n",
      "Iteration 232, loss = 5.71063496\n",
      "Iteration 233, loss = 5.57618933\n",
      "Iteration 234, loss = 5.44465812\n",
      "Iteration 235, loss = 5.31917161\n",
      "Iteration 236, loss = 5.19619287\n",
      "Iteration 237, loss = 5.07777779\n",
      "Iteration 238, loss = 4.96218501\n",
      "Iteration 239, loss = 4.85133699\n",
      "Iteration 240, loss = 4.73871557\n",
      "Iteration 241, loss = 4.63416768\n",
      "Iteration 242, loss = 4.53300216\n",
      "Iteration 243, loss = 4.43097735\n",
      "Iteration 244, loss = 4.33332057\n",
      "Iteration 245, loss = 4.24226066\n",
      "Iteration 246, loss = 4.14984940\n",
      "Iteration 247, loss = 4.06193402\n",
      "Iteration 248, loss = 3.97504241\n",
      "Iteration 249, loss = 3.89711403\n",
      "Iteration 250, loss = 3.81455310\n",
      "Iteration 251, loss = 3.73618152\n",
      "Iteration 252, loss = 3.66209552\n",
      "Iteration 253, loss = 3.59005732\n",
      "Iteration 254, loss = 3.51994372\n",
      "Iteration 255, loss = 3.45082431\n",
      "Iteration 256, loss = 3.38567126\n",
      "Iteration 257, loss = 3.32211643\n",
      "Iteration 258, loss = 3.25809176\n",
      "Iteration 259, loss = 3.19905311\n",
      "Iteration 260, loss = 3.14200724\n",
      "Iteration 261, loss = 3.08441786\n",
      "Iteration 262, loss = 3.03094874\n",
      "Iteration 263, loss = 2.97672108\n",
      "Iteration 264, loss = 2.92670966\n",
      "Iteration 265, loss = 2.87801039\n",
      "Iteration 266, loss = 2.82997614\n",
      "Iteration 267, loss = 2.78220881\n",
      "Iteration 268, loss = 2.73822454\n",
      "Iteration 269, loss = 2.69426884\n",
      "Iteration 270, loss = 2.65274660\n",
      "Iteration 271, loss = 2.61267316\n",
      "Iteration 272, loss = 2.57176648\n",
      "Iteration 273, loss = 2.53487493\n",
      "Iteration 274, loss = 2.49780511\n",
      "Iteration 275, loss = 2.46289121\n",
      "Iteration 276, loss = 2.42813016\n",
      "Iteration 277, loss = 2.39423078\n",
      "Iteration 278, loss = 2.36324997\n",
      "Iteration 279, loss = 2.33118099\n",
      "Iteration 280, loss = 2.30098970\n",
      "Iteration 281, loss = 2.27295212\n",
      "Iteration 282, loss = 2.24435546\n",
      "Iteration 283, loss = 2.21823911\n",
      "Iteration 284, loss = 2.19067987\n",
      "Iteration 285, loss = 2.16620949\n",
      "Iteration 286, loss = 2.14201316\n",
      "Iteration 287, loss = 2.11911148\n",
      "Iteration 288, loss = 2.09549298\n",
      "Iteration 289, loss = 2.07419808\n",
      "Iteration 290, loss = 2.05356415\n",
      "Iteration 291, loss = 2.03272389\n",
      "Iteration 292, loss = 2.01272623\n",
      "Iteration 293, loss = 1.99445641\n",
      "Iteration 294, loss = 1.97526649\n",
      "Iteration 295, loss = 1.95763082\n",
      "Iteration 296, loss = 1.93988765\n",
      "Iteration 297, loss = 1.92356439\n",
      "Iteration 298, loss = 1.90746923\n",
      "Iteration 299, loss = 1.89226049\n",
      "Iteration 300, loss = 1.87756519\n",
      "Iteration 301, loss = 1.86229556\n",
      "Iteration 302, loss = 1.84876229\n",
      "Iteration 303, loss = 1.83541642\n",
      "Iteration 304, loss = 1.82191446\n",
      "Iteration 305, loss = 1.80948355\n",
      "Iteration 306, loss = 1.79750374\n",
      "Iteration 307, loss = 1.78533436\n",
      "Iteration 308, loss = 1.77361895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 309, loss = 1.76361722\n",
      "Iteration 310, loss = 1.75239578\n",
      "Iteration 311, loss = 1.74217582\n",
      "Iteration 312, loss = 1.73233570\n",
      "Iteration 313, loss = 1.72218428\n",
      "Iteration 314, loss = 1.71292222\n",
      "Iteration 315, loss = 1.70420056\n",
      "Iteration 316, loss = 1.69537127\n",
      "Iteration 317, loss = 1.68704564\n",
      "Iteration 318, loss = 1.67904849\n",
      "Iteration 319, loss = 1.67066304\n",
      "Iteration 320, loss = 1.66344366\n",
      "Iteration 321, loss = 1.65613086\n",
      "Iteration 322, loss = 1.64845130\n",
      "Iteration 323, loss = 1.64179330\n",
      "Iteration 324, loss = 1.63527411\n",
      "Iteration 325, loss = 1.62890347\n",
      "Iteration 326, loss = 1.62214046\n",
      "Iteration 327, loss = 1.61598888\n",
      "Iteration 328, loss = 1.61037203\n",
      "Iteration 329, loss = 1.60409478\n",
      "Iteration 330, loss = 1.59912844\n",
      "Iteration 331, loss = 1.59301975\n",
      "Iteration 332, loss = 1.58811885\n",
      "Iteration 333, loss = 1.58289647\n",
      "Iteration 334, loss = 1.57798794\n",
      "Iteration 335, loss = 1.57297802\n",
      "Iteration 336, loss = 1.56810917\n",
      "Iteration 337, loss = 1.56375606\n",
      "Iteration 338, loss = 1.55900610\n",
      "Iteration 339, loss = 1.55477495\n",
      "Iteration 340, loss = 1.55066079\n",
      "Iteration 341, loss = 1.54662869\n",
      "Iteration 342, loss = 1.54236183\n",
      "Iteration 343, loss = 1.53873296\n",
      "Iteration 344, loss = 1.53458574\n",
      "Iteration 345, loss = 1.53094485\n",
      "Iteration 346, loss = 1.52705455\n",
      "Iteration 347, loss = 1.52380558\n",
      "Iteration 348, loss = 1.52010210\n",
      "Iteration 349, loss = 1.51675053\n",
      "Iteration 350, loss = 1.51358593\n",
      "Iteration 351, loss = 1.51019550\n",
      "Iteration 352, loss = 1.50716609\n",
      "Iteration 353, loss = 1.50400142\n",
      "Iteration 354, loss = 1.50120179\n",
      "Iteration 355, loss = 1.49808269\n",
      "Iteration 356, loss = 1.49509986\n",
      "Iteration 357, loss = 1.49226742\n",
      "Iteration 358, loss = 1.48939850\n",
      "Iteration 359, loss = 1.48667303\n",
      "Iteration 360, loss = 1.48407569\n",
      "Iteration 361, loss = 1.48128116\n",
      "Iteration 362, loss = 1.47852758\n",
      "Iteration 363, loss = 1.47591405\n",
      "Iteration 364, loss = 1.47320479\n",
      "Iteration 365, loss = 1.47096841\n",
      "Iteration 366, loss = 1.46824212\n",
      "Iteration 367, loss = 1.46577666\n",
      "Iteration 368, loss = 1.46319324\n",
      "Iteration 369, loss = 1.46103298\n",
      "Iteration 370, loss = 1.45851273\n",
      "Iteration 371, loss = 1.45623717\n",
      "Iteration 372, loss = 1.45373900\n",
      "Iteration 373, loss = 1.45145352\n",
      "Iteration 374, loss = 1.44917265\n",
      "Iteration 375, loss = 1.44695052\n",
      "Iteration 376, loss = 1.44466819\n",
      "Iteration 377, loss = 1.44251930\n",
      "Iteration 378, loss = 1.44032623\n",
      "Iteration 379, loss = 1.43807047\n",
      "Iteration 380, loss = 1.43620392\n",
      "Iteration 381, loss = 1.43381974\n",
      "Iteration 382, loss = 1.43178442\n",
      "Iteration 383, loss = 1.42981918\n",
      "Iteration 384, loss = 1.42774256\n",
      "Iteration 385, loss = 1.42566651\n",
      "Iteration 386, loss = 1.42359254\n",
      "Iteration 387, loss = 1.42168078\n",
      "Iteration 388, loss = 1.41961488\n",
      "Iteration 389, loss = 1.41766485\n",
      "Iteration 390, loss = 1.41570730\n",
      "Iteration 391, loss = 1.41364626\n",
      "Iteration 392, loss = 1.41166734\n",
      "Iteration 393, loss = 1.40983522\n",
      "Iteration 394, loss = 1.40794249\n",
      "Iteration 395, loss = 1.40599404\n",
      "Iteration 396, loss = 1.40400853\n",
      "Iteration 397, loss = 1.40208099\n",
      "Iteration 398, loss = 1.40021654\n",
      "Iteration 399, loss = 1.39833319\n",
      "Iteration 400, loss = 1.39647774\n",
      "Iteration 401, loss = 1.39469219\n",
      "Iteration 402, loss = 1.39266063\n",
      "Iteration 403, loss = 1.39072336\n",
      "Iteration 404, loss = 1.38897747\n",
      "Iteration 405, loss = 1.38727456\n",
      "Iteration 406, loss = 1.38509719\n",
      "Iteration 407, loss = 1.38339860\n",
      "Iteration 408, loss = 1.38149839\n",
      "Iteration 409, loss = 1.37978474\n",
      "Iteration 410, loss = 1.37785234\n",
      "Iteration 411, loss = 1.37591096\n",
      "Iteration 412, loss = 1.37424400\n",
      "Iteration 413, loss = 1.37231524\n",
      "Iteration 414, loss = 1.37043879\n",
      "Iteration 415, loss = 1.36862523\n",
      "Iteration 416, loss = 1.36665669\n",
      "Iteration 417, loss = 1.36484766\n",
      "Iteration 418, loss = 1.36287660\n",
      "Iteration 419, loss = 1.36116009\n",
      "Iteration 420, loss = 1.35920872\n",
      "Iteration 421, loss = 1.35726242\n",
      "Iteration 422, loss = 1.35540491\n",
      "Iteration 423, loss = 1.35349468\n",
      "Iteration 424, loss = 1.35176853\n",
      "Iteration 425, loss = 1.34980891\n",
      "Iteration 426, loss = 1.34803058\n",
      "Iteration 427, loss = 1.34605970\n",
      "Iteration 428, loss = 1.34416857\n",
      "Iteration 429, loss = 1.34241047\n",
      "Iteration 430, loss = 1.34088259\n",
      "Iteration 431, loss = 1.33885473\n",
      "Iteration 432, loss = 1.33687213\n",
      "Iteration 433, loss = 1.33501907\n",
      "Iteration 434, loss = 1.33332581\n",
      "Iteration 435, loss = 1.33148322\n",
      "Iteration 436, loss = 1.32973778\n",
      "Iteration 437, loss = 1.32776957\n",
      "Iteration 438, loss = 1.32628199\n",
      "Iteration 439, loss = 1.32423356\n",
      "Iteration 440, loss = 1.32236600\n",
      "Iteration 441, loss = 1.32066916\n",
      "Iteration 442, loss = 1.31880128\n",
      "Iteration 443, loss = 1.31690728\n",
      "Iteration 444, loss = 1.31531627\n",
      "Iteration 445, loss = 1.31323030\n",
      "Iteration 446, loss = 1.31176094\n",
      "Iteration 447, loss = 1.30985934\n",
      "Iteration 448, loss = 1.30788739\n",
      "Iteration 449, loss = 1.30610974\n",
      "Iteration 450, loss = 1.30439220\n",
      "Iteration 451, loss = 1.30267015\n",
      "Iteration 452, loss = 1.30086434\n",
      "Iteration 453, loss = 1.29892422\n",
      "Iteration 454, loss = 1.29719649\n",
      "Iteration 455, loss = 1.29552544\n",
      "Iteration 456, loss = 1.29362558\n",
      "Iteration 457, loss = 1.29193164\n",
      "Iteration 458, loss = 1.29001730\n",
      "Iteration 459, loss = 1.28823600\n",
      "Iteration 460, loss = 1.28652486\n",
      "Iteration 461, loss = 1.28484333\n",
      "Iteration 462, loss = 1.28289973\n",
      "Iteration 463, loss = 1.28103357\n",
      "Iteration 464, loss = 1.27924209\n",
      "Iteration 465, loss = 1.27750832\n",
      "Iteration 466, loss = 1.27573220\n",
      "Iteration 467, loss = 1.27387512\n",
      "Iteration 468, loss = 1.27220463\n",
      "Iteration 469, loss = 1.27032216\n",
      "Iteration 470, loss = 1.26856882\n",
      "Iteration 471, loss = 1.26679423\n",
      "Iteration 472, loss = 1.26503430\n",
      "Iteration 473, loss = 1.26322476\n",
      "Iteration 474, loss = 1.26168557\n",
      "Iteration 475, loss = 1.25971588\n",
      "Iteration 476, loss = 1.25800478\n",
      "Iteration 477, loss = 1.25614247\n",
      "Iteration 478, loss = 1.25442350\n",
      "Iteration 479, loss = 1.25270916\n",
      "Iteration 480, loss = 1.25094146\n",
      "Iteration 481, loss = 1.24924826\n",
      "Iteration 482, loss = 1.24751303\n",
      "Iteration 483, loss = 1.24558350\n",
      "Iteration 484, loss = 1.24390500\n",
      "Iteration 485, loss = 1.24217945\n",
      "Iteration 486, loss = 1.24057468\n",
      "Iteration 487, loss = 1.23858576\n",
      "Iteration 488, loss = 1.23686322\n",
      "Iteration 489, loss = 1.23522003\n",
      "Iteration 490, loss = 1.23335490\n",
      "Iteration 491, loss = 1.23167023\n",
      "Iteration 492, loss = 1.23022666\n",
      "Iteration 493, loss = 1.22817272\n",
      "Iteration 494, loss = 1.22632791\n",
      "Iteration 495, loss = 1.22456230\n",
      "Iteration 496, loss = 1.22287611\n",
      "Iteration 497, loss = 1.22107482\n",
      "Iteration 498, loss = 1.21934503\n",
      "Iteration 499, loss = 1.21754754\n",
      "Iteration 500, loss = 1.21574297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/casaponsa/Library/Python/3.8/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=10, max_iter=500, verbose=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train) # usa modelo red neuronal que hemos definido antes y(x,w)\n",
    "# mlp.loss_curve en Clasificación te guarda el valor de la función coste a cada iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss function')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "pl.plot(mlp.loss_curve_)  # mlp.loss_curve nos da una idea de como converge el training set\n",
    "pl.xlabel('iteration')\n",
    "pl.ylabel('loss function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos las predicciones con el set de validación y hacemos algún cálculo para ver si se parecen con los valores reales. Aquí calculo el coeficiente de correlación (que debería ser 1) y el error relativo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeficiente correlación 0.999354649801217\n",
      "error relativo 0.7281247567830477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1243d6e50>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test) # estimamos resultado con el TEST\n",
    "print('coeficiente correlación', np.corrcoef(predictions,y_test)[0][1] ) # calculamos correlación con el valor esperado\n",
    "print('error relativo', np.mean(abs(predictions-y_test)/y_test)*100)  # calculamos correlación con el valor esperado\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "pl.plot(predictions,y_test,'.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pido a la red que sume 10 + 2 + 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma predicha es [18.]\n",
      "la suma real es 18\n"
     ]
    }
   ],
   "source": [
    "A=np.array([10, 2, 6]).reshape(1,-1) # sklearn necesita que se lo de dimension(1,n)\n",
    "print('la suma predicha es', np.round(mlp.predict(A))) # redondamos\n",
    "print('la suma real es',np.round(np.sum(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pido a la red que sume 100 + 100 + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma predicha es [299.]\n",
      "la suma real es 300\n"
     ]
    }
   ],
   "source": [
    "# si miramos valores muy lejos del rango\n",
    "A=np.array([100,100,100]).reshape(1,-1) \n",
    "np.shape(A)\n",
    "print('la suma predicha es', np.round(mlp.predict(A))) # redondamos\n",
    "print('la suma real es',np.round(np.sum(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora Le pido a la red que sume 1000 + 1000 + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma predicha es [7019.]\n",
      "la suma real es 7050\n"
     ]
    }
   ],
   "source": [
    "# si miramos valores muy lejos del rango\n",
    "A=np.array([2000,1050,4000]).reshape(1,-1) \n",
    "np.shape(A)\n",
    "print('la suma predicha es', np.round(mlp.predict(A))) # redondamos\n",
    "print('la suma real es',np.round(np.sum(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NO ha aprendido a sumar!** Ha ajustado muy bien los datos pero a la que me alejo del rango (0,100) se equivoca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios: \n",
    "\n",
    "1. Haz pruebas con esta red para ver si mejoras los resultados. Para mejorar un ajuste puedes probar:\n",
    "\n",
    "* cambiar el número de neuronas o capas \n",
    "\n",
    "* Parámetros del MPLRegressor como función de activación, optimizer, loss_function, batch_size,...\n",
    "\n",
    "* Añadir más datos, usamos cuantos más datos mejor, hasta que veamos que añadir más datos no mejora la solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Entrena una red para que aprenda a multiplicar 4 números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
